#Note: you need to clone this repo before running it:https://github.com/tensorflow/tpu.git

from __future__ import print_function
from IPython import display
import  eval_ckpt_main as eval_ckpt
import tensorflow as tf
import json
import os
import sys
sys.path.append('/content/tpu/models/official/efficientnet')
sys.path.append('/content/tpu/models/common')


model_name = 'efficientnet-b0' #@param

image_folder = "/home/oyku/texas/ILSVRC2012_img_val"

# Set up the paths and filenames
model_name = 'efficientnet-b0'
ckpt_dir = "/home/oyku/texas/tpu/models/official/efficientnet/efficientnet-b0"
labels_map_file = "/home/oyku/texas/tpu/models/official/efficientnet/labelsmap.txt"
labels_file = "/home/oyku/texas/tpu/models/official/efficientnet/imagenet_metadata.txt"
ground_truth_labels_file = "/home/oyku/texas/tpu/models/official/efficientnet/2012_validation_labels.txt"
image_folder = "/home/oyku/texas/ILSVRC2012_img_val"


# Load labels map for predictions
with open(labels_map_file, "r") as f:
    labels_map = json.load(f)

file_to_label_mapping = {}
with open(labels_file, 'r') as f:
    for line in f.readlines():
        file_name, label = line.strip().split('\t')
        file_to_label_mapping[file_name] = label


validation_labels = []
with open(ground_truth_labels_file, 'r') as f:
    for line in f.readlines():
        label = line.strip()
        if label in file_to_label_mapping:
            validation_labels.append(label)
        else:
            print(f"Skipping validation label {label} as its image couldn't be opened.")

# Initialize variables to track accuracy
total_images = len(validation_labels)
correct_predictions = 0

# Get a list of image filenames
image_filenames = sorted([filename for filename in os.listdir(image_folder) if filename.endswith('.JPEG')])

i = 0

# Evaluate each image and compare predictions with ground truth
for image_file in image_filenames:
    i += 1
    image_path = os.path.join(image_folder, image_file)
    image_files = [image_path]
    eval_driver = eval_ckpt.get_eval_driver(model_name)
    pred_idx, pred_prob = eval_driver.eval_example_images(ckpt_dir, image_files, labels_map_file)

    # Get the top 5 predictions
    top5_indices = pred_idx[0][:5]
    top5_labels = [labels_map[str(idx)] for idx in top5_indices]

    # Get the ground truth label for the image
    ground_truth_label = validation_labels[i-1]
    print(ground_truth_label)

    # Map the ground truth label to its correspondence in imagenet_metadata.txt
    ground_truth_label_mapped = file_to_label_mapping[ground_truth_label]
    print(ground_truth_label_mapped)

    if ground_truth_label_mapped is not None:
        # Check if the ground truth label is in the top 5 predictions
        if ground_truth_label_mapped in top5_labels:
            correct_predictions += 1
        print(correct_predictions)
    else:
        # Skip the image if the label is not found in the mapping
        print(f"Skipping image {image_file} as the corresponding label could not be found in the mapping.")

    print("Image {}/{} - Ground Truth: {}, Corresponding Label: {}".format(i, total_images, ground_truth_label, ground_truth_label_mapped))

# Calculate accuracy
accuracy = correct_predictions / total_images

# Save accuracy to a text file
with open("accuracy.txt", "w") as f:
    f.write(str(accuracy))

# Print the accuracy
print("Accuracy:", accuracy)
